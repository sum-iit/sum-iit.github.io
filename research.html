<!-- research.html -->
<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research - Sumit Sah</title>
    <link rel="stylesheet" href="assets/css/style.css">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600&display=swap" rel="stylesheet">
</head>
<body>
    <main class="main">
        <div class="container">
            <div class="content-wrapper">
                <div class="profile-section"></div>
                <div class="bio-section">
                    <h3 class="section-title">Research Interests</h3>
                    <p>My research focuses on the theoretical underpinnings of federated and decentralized learning, with a particular emphasis on developing and analyzing optimization algorithms. I am interested in understanding the convergence properties and generalization capabilities of these methods under realistic and challenging conditions, such as non-convex loss landscapes and data heterogeneity.</p>



                    <h3 class="section-title">Publications</h3>
                    
                    <div class="publication">
                        <h4 class="pub-title">Online Learning with Non-convex Losses: New Condition to Achieve Small Dynamic Regret</h4>
                        <p class="pub-authors">Sumit Sah, Bharath B.N</p>
                        <p class="pub-venue">IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), Hyderabad, India, 2025</p>
                        <p class="pub-description">We study online learning with non-convex, time-varying loss functions. We show that Online Gradient Descent (OGD) can achieve dynamic regret comparable to the strongly convex case, under a novel condition defined within a ball of radius p around the initialization. This condition incorporates gradients, losses, and temporal variation to reflect the online nature of the problem. Under this assumption, OGD attains dynamic regret scaling sub-linearly with variation. Experimental results support our theory</p>
                    </div>

                    <div class="publication">
                        <h4 class="pub-title">Generalization of FedAvg Under Constrained Polyak-Łojasiewicz Type Conditions: A Single Hidden Layer Neural Network Analysis</h4>
                        <p class="pub-authors">Sumit Sah, Shruti P Maralappanavar, B. N. Bharath, Prashant Khanduri</p>
                        <p class="pub-venue">Submitted</p>
                        <p class="pub-description">We study the FedAvg algorithm in Federated Learning, focusing on its optimization and generalization performance. Under new constrained conditions, we show that FedAvg converges linearly and applies to certain neural networks with sufficient width. We also prove that its generalization error improves optimally with more data and benefits further from having more clients.</p>
                    </div>

                    <div class="publication">
                        <h4 class="pub-title">Localized Growth Conditions for Decentralized FedAvg: Convergence to Global Optimal Points</h4>
                        <p class="pub-authors">Sumit Sah, Bharath B.N</p>
                        <p class="pub-venue">Submitted</p>
                        <p class="pub-description">We prove the first linear convergence guarantee for Decentralized Federated Averaging (D-FedAvg) under a local PL-type growth condition, far weaker than standard global assumptions. This setting requires neither data homogeneity nor global minimizers—these properties emerge naturally as iterates stay in a bounded region. Our analysis introduces new drift-dynamics bounds to control consensus error, and experiments on diverse datasets confirm the theory</p>
                    </div>

                    <h3 class="section-title">Current Projects</h3>
                    <p>I am currently working on several research projects that bridge the gap between academic research and industry applications. These projects involve collaboration with leading technology companies and academic institutions, focusing on next-generation distributed systems and AI integration patterns.</p>
                </div>
            </div>
        </div>
    </main>
    <script src="assets/js/config.js"></script>
    <script src="assets/js/script.js"></script>
</body>
</html>
